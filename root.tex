%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
\usepackage{graphicx}

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{mathtools}

\title{\LARGE \bf
CSE 598 Project Proposal: \\
Imitation Learning with Baxter Robot using Hi-Fives
}

\author{Michael Drolet, Frankie Liu, Evan Lam}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The goal of this project is to successfully learn hi-fives for human-robot interaction. We used an Imitation Learning approach by incorporating Bayesian Interaction Primitives \cite{c1}. Through expert-guided demonstrations, we train the robot to learn relationships between human and robot trajectories. We demonstrate that the robot is able to complete the interaction with a human and successfully issue a hi-five. We implemented the Bayesian Interaction Primitives to teach a Baxter Robot how to hi-five through imitation learning. Additionally, we compared the trajectories with human bio-mechanics data.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Teaching robots how to interact with humans is an interesting challenge in the field of Human Robot Interaction. The athletic and social components which make humans successful in daily interactions are often difficult to capture for robots. Nonetheless there are a variety of methods to teach robots to learn new tasks for human interaction. Imitation learning uses a human expert to guide the robot's interaction. Given the current situation, people are more socially isolated than ever. We hope that adding human interactions to robots can improve a person's overall well-being.
\newline
\indent Our goal is to have a simple interaction that is able to be generalized across different robots using limited samples. The Bayesian Interaction approach enables a simple interaction which can be generalized using limited samples.
\newline
\indent For our project, we train a Baxter robot which is traditionally an industrial robot built by Rethink Robotics. This Baxter robot is refitted in a friendly bear costume. We hope that by developing interactions for this Baxter robot, we can create an emotional support robot which can provide people comfort in times of emotional distress.
\newline
\begin{figure}[h]
\centering
\includegraphics[width=0.2\textwidth]{Bear.PNG}
\caption{Example of human hi-fiving Baxter}
\end{figure}

\section{Related Work}

\subsection{Bayesian Interaction Primitives}
The Bayesian Interaction Primitive (BIP) framework is a novel and powerful approach that is being used by the Interactive Robotics Lab at Arizona State University. This SLAM inspired algorithm is useful for encoding spatiotemporal information about the interaction into a state space. BIP approximates each dimension using a weighted linear combination of time-dependent basis functions. In this case, we are using Gaussian basis functions. Included in a state vector are the weights for every basis function, the phase, and phase velocity. As a result, BIP allows us to infer the current state, given previous states (demonstrations) and current observations. Formatting this probabilistically allows the inference to be done using Bayes Filters (in particular, a Kalman Filter) during testing to guide the robot. Since the robot's degrees of freedom are no longer being observed during testing, the BIP framework is based on obtaining a partial observation of the current state to generate the positions/variables of the whole state space. This approach is effective for learning on different types of robots, as there is no domain specific knowledge encoded into the algorithm.

\section{Problem Statement}
The goal of this project is for the Baxter robot to successfully issue hi-fives. Our design consideration consists of a quick hi-five with a high success rate. We did most of our training and demonstrations on the Baxter robot, we also integrate our work with the RViz and VREP simulation tool.

\subsection{Teaching Baxter}
To teach Baxter, we utilized the Intprim library. Through the process, we utilized methods for implement an extended Kalman filter and evaluated different paramater settings to fit the best moel. We also utilized methods for basis function decomposition, where we received weights and selected the number of gaussians needed. We also set up custom paramater files to specify generated DOFs and work with the various devices (relevant to our project). Lastly, we utilized methods for training, testing, and working with data manipulation. The data was recorded as rosbags, and much of our analysis was done inside a docker container to sanitize data and evaluate results.

We trained two different primitives: a slow hi-five and a faster (more realistic) hi-five for our experiments. The Intrpim library was used to generate a model for our different primitives. We then evaluated the performance of each primitive and tested the models. The first model, which had more training examples, did much better than the second model. One of the issues encountered with the second model was that the rosbags were too long, so there was too much noise (only the first half was relevant). After trimming the bags to the correct length, we got worse performance. This leads us to believe that the bags were incorrectly trimmed, or something erreneous happened along the way. However, trimming the first set bags to the correct length improved the performance greatly. With that being said, our first interaction was a success. Due to the current cirumstances, we weren't able to verify whether the second interaction can be improved, as we need the physical robot to collect new training samples.

We used the Extended Kalman Filter (EKF) to perform the filtering in the experiment, as this filter is suitable for smaller sample sizes (compared to the Extended Kalman Filter) and offers good performance in practice.

\subsection{Data Generation}
We collected subject data by using the Optitrack motion capture system, where 3D poses were collected at 120 Hz. Subjects wore a headband and two gloves that are equipped with motion capture markers. By using only three degrees of freedom, we avoid performing inference in a space that is too high-dimensional. After all, hands and head position are likely the only relevant information needed to perform a successful hi-five. We used ROS as our communication protocol and to handle messages.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{Optitracker.jpg}
\caption{Optitracker application running}
\end{figure}

\subsection{Implementation}
A huge advantage of using the in-person Baxter robot is that we are able to find joint angle key frames for the training pose easily. For example, we put the robot into puppet mode and then recorded its key positions for a hi-five. Then, all that is required is to interpolate between these points to obtain a motion sequence. This method allowed us to quickly create different hi-five poses and allows us to come up with new poses if we decide to improve our hi-five motion. Lastly, the biggest benefit of using the Baxter in-person is that we have access to a controlled environment- with an Optitrack system, high speed network, and device drivers all configured- so that development and deployment is seamless.
\newline
\indent In order to train the Baxter robot, our group conducted approximately 75 in-person demonstrations (hi-fives) on the Baxter robot. Most of these test hi-fives were done using Frankie as the HRI partner, however we did not want our model to overfit the trajectories by using only one person. Therefore, we decided to also use Evan as a HRI partner during training. The hi-five was generated using key frames. We trained the Baxter robot on two different hi-five interaction types. Our first hi-five was very segmented and tried to mimic a robot movement. Our second hi-five was more fluid and smooth like a natural hi-five between two people. 

\subsection{Interaction Analysis and Bio-mechanic Analysis}

Current cirumstances forced us to use the simulators to evaluate our training examples. Because of this, we were unable to record HRI samples and had a limited amount of training data. However, we utilized all of the training data to the best of our extent and experimented with different parameters. As mentioned above, the first hi-five interaction was successful during testing. We believe this is because there was more training data available, and the data was sanitized better. The second set of training data (interaction 2) had examples that were too long and noisy. Ultimately, we reached an accuracy of approximately 71 percent on the first set of training data. This was through visual inspection, as there was no other obvious way to calculate the success besides a human-determined value.
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{biomechanics.png}
\caption{Human-human hi-five}
\end{figure} 
\newline
\indent From the graphs, we can see that the y-axis in this case likely represents the vertical axis, as the position of both participants wrists within the y-axis are almost the same throughout the entire duration of the hi-five. In a natural hi-five between two people, the participants both move their hands across their body to meet at a center point, resulting in the significant movement in the x-axis and z-axis, towards a single position, seen in the graphs in Fig. 3.
\newline
\indent Next, we analyzed the data captured from a hi-five between a person and the robot using the initial hi-five model that we developed. Optitrack sensors were placed on the participant's head, right wrist, and left wrist. Data for the Baxter robot was collected directly from it using the respective ROS topics. The three graphs in Fig. 4 show the x,y and z positions of the human's right wrist, and the robots, right "hand".
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{oldfive.png}
\caption{Old human-robot hi-five. Note: data after about 1250 on time axis is extraneous}
\end{figure}
\newline
\indent The first thing to note here is that the data for the human is reported by the Optitrac system while the data for the robot is reported by the Baxter itself. This results in the points being mapped with respect to different origins, as shown in the y-axis graph, which once again represents the vertical axis, since the max height of the human's wrist is show to be significantly higher than the max height of the robot's "hand" despite the fact that they are at the same physical height at the time of the hi-five (around 800 on the time axis). This also causes the x-axis and y-axis graphs to look significantly different from the graphs generated from the human-human hi-five. 
\newline
\indent The overall movement patterns of each participant of the human-robot hi-five largely match the movement patterns of each participant of the human-human hi-five, even if the positions do not necessarily match up do to the different origin points as previously explained. While the overall patterns do match, the movement is much more jagged due to the stiff nature of the first hi-five model that we created.
\newline
\indent Finally, we analyzed the data captured from a hi-five between a person and the Baxter robot using the revised hi-five model that we developed. The same setup was used as the data collected from the original human-robot hi-five. The three graphs in Fig. 5 show the x,y and z positions of the human's right wrist, and the robots, right "hand".
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{newfive.png}
\caption{New human-robot hi-five}
\end{figure}
\newline
\indent Once again, the origin points used by the Optitrac system and the Baxter robot are different, resulting in the graphs looking significantly different from the human-human hi-five graphs. However, the overall movement patterns of each participant for the new human-robot hi-five match the movements of the participants in the human-human hi-five. With the new hi-five model, we also noticed that the movement within each axis was much more smooth compared to the movements from the old hi-five model, indicating our new model completes the hi-five in a more natural way that better mimics an actual human hi-five.
\newline
\indent To get a better comparison between the new robot hi-five model and an actual hi-five between two humans, we computed the correlation coefficients comparing the robot from the human-robot hi-five (new model) to the first human participant in the human-human hi-five:
\[
corrcoef(x)_{robot, human_1} = \begin{bmatrix}
	1 & -0.45045106 \\
	-0.45045106 & 1
\end{bmatrix}
\]
\newline
\[
corrcoef(y)_{robot, human_1} = \begin{bmatrix}
	1 & 0.53274616 \\
	0.53274616 & 1
\end{bmatrix}
\]
\newline
\[
corrcoef(z)_{robot, human_1} = \begin{bmatrix}
	1 & 0.25234082 \\
	0.25234082 & 1
\end{bmatrix}
\]
\newline
We also computed the correlation coefficients comparing the human from the human-robot hi-five (new model) to the second human participant in the human-human hi-five:
\[
corrcoef(x)_{human, human_2} = \begin{bmatrix}
1 & -0.31119984 \\
-0.31119984 & 1
\end{bmatrix}
\]
\newline
\[
corrcoef(y)_{human, human_2} = \begin{bmatrix}
1 & 0.56795077 \\
0.56795077 & 1
\end{bmatrix}
\]
\newline
\[
corrcoef(z)_{human, human_2} = \begin{bmatrix}
1 & -0.55262954 \\
-0.55262954 & 1
\end{bmatrix}
\]
\newline
From the above correlation coefficient matrices we see that the corresponding co-variance values from each axis in the robot to human1 comparison matrices is fairly similar to the co-variance values from each axis in the human to human2 comparison matrices. This indicates to us that our interaction with the robot is similar to the interactions between the humans. Therefore, the second hi-five model that we designed is good at matching a natural hi-five between two humans.

\subsection{Simulation Integration} 
The simulation tool we were initially using was VREP. This was because it is user-friendly and there are useful resources to begin developing with it. We were considering using Gazebo due to its ability to integrate tightly with ROS. However, Gazebo appeared to be heavy compared to other simulators. We ultimately decided on a custom implemented RViz client integrated in our container. 
\newline
\indent
For our simulations, we used an RViz client inside the container. This client subscribes to TF topics in ROS so that it can update the robot's transformation information. In order to get this working, we used a couple of different libraries. For doing training, we used a joint state library that reads from the URDF file and publishes the joint states to a robot state library. The robot state library then converts this into a TF topic, so that RViz and our Jupyter client can render the robot's state. For inference, we instead redirected the joint command outputs from intprim to a joint state topic. This joint state topic was then sent to the robot state libarary, where the information was converted to TF topic for the notebooks to use. This allowed us to visualize the inference and helped us validate our training data. For the presentations, we prepared a notebook to showcase the simulation and inference with our different types of interactions. If it wasn't for the simulation, we wouldn't have realized that the rosbags were incorrectly recorded (initially) either.
\newline
\indent We used the docker container and custom simulation (that we made from libraries and intprim integration) to analyze many aspects of the project. For example, all of the rosbags were evaluated using the simulation tool and then we also evaluated inference. The biomechanical analysis was also done in our container. Since we all had the same environment, development was much easier and we didn't have different versions to worry about. The simulation wasn't used to train the robot, since we didn't have the optitrack system or kinect available, but it was a great way to verify our results and work in a new environment.


\section{Current Limitations and Future Work}
Some of the current limitations of this project are that the Hi-5s are still a bit slow and we can work on improving it. The Machine Learning Model needs improvement. Perhaps the model can be improved with better training data, or parameter exploration. For future work, we would like to apply the learning model onto various other robots and also explore a wide variety of Hi-5 interactions/gestures.

\section{Conclusion}
In conclusion we taught a Baxter Robot to Hi-5. We had both a real life implementation and a simulated implementation. We finished writing the ROS subscribers and RViz interfaces in the simulation container. We compared the robot movement with human biomechanical analysis. We hope to continue to improve the human-robot interaction and expand this to generalize to other interactions improving human well being through robot interactions.

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{99}
\bibitem{c1} J. Campbell, A. Hitzmann, S. Stepputtis, S. Ikemoto, K. Hosoda, and H. Ben Amor. Learning Interactive Behaviors for Musculoskeletal Robots Using Bayesian Interaction Primitives. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, November 2019.
\end{thebibliography}




\end{document}
